# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: MIT-0

apiVersion: v1
kind: ServiceAccount
metadata:
  name: spark
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: spark-role
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: edit
subjects:
  - kind: ServiceAccount
    name: spark
    namespace: default
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: tpcds-data-generation-5tb-template
data:
  driver: |-
    apiVersion: v1
    kind: Pod
    spec:
      nodeSelector:
        lifecycle: od
      initContainers:
      - name: volume-permissions
        image: public.ecr.aws/y4g4v0z7/busybox
        command: ['sh', '-c', 'chown -R 185 /pv/tmp']
        volumeMounts:
        - mountPath: /pv/tmp
          name: spark-local-dir-1

  executor: |-
    apiVersion: v1
    kind: Pod
    spec:
      initContainers:
      - name: volume-permissions
        image: public.ecr.aws/y4g4v0z7/busybox
        command: ['sh', '-c', 'chown -R 185 /pv/tmp']
        volumeMounts:
        - mountPath: /pv/tmp
          name: spark-local-dir-1
      tolerations:
      - key: "spot"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
      nodeSelector:
        lifecycle: spot
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: spark-app
                operator: In
                values:
                - tpcds-data-generation-5tb
            topologyKey: failure-domain.beta.kubernetes.io/zone

---
apiVersion: batch/v1
kind: Job
metadata:
  name: tpcds-data-generation-5tb
spec:
  template:
    spec:
      containers:
        - name: spark
          # Standard image (without Spark Hadoop Cloud libs) can be used because we just need the spark-submit CLI
          image: public.ecr.aws/y4g4v0z7/spark
          command: [
            "/bin/sh",
            "-c",
            "/opt/spark/bin/spark-submit \
            --master k8s://https://kubernetes.default.svc.cluster.local:443 \
            --deploy-mode cluster \
            --name tpcds-data-generation-5tb \
            --class com.amazonaws.eks.tpcds.DataGeneration \
            --conf spark.dynamicAllocation.enabled=true \
            --conf spark.dynamicAllocation.shuffleTracking.enabled=true \
            --conf spark.kubernetes.allocation.batch.size=200 \
            --conf spark.driver.cores=3 \
            --conf spark.driver.memory=24G \
            --conf spark.executor.memory=27G \
            --conf spark.executor.cores=3 \
            --conf spark.sql.shuffle.partitions=2000 \
            --conf spark.kubernetes.container.image=public.ecr.aws/y4g4v0z7/eks-spark-benchmark:v3.0.1 \
            --conf spark.kubernetes.container.image.pullPolicy=Always \
            --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
            --conf spark.kubernetes.driver.label.spark-app=tpcds-data-generation-5tb \
            --conf spark.kubernetes.node.selector.noderole=spark \
            --conf spark.kubernetes.node.selector.os=linux \
            --conf spark.kubernetes.node.selector.arch=intel \
            --conf spark.kubernetes.node.selector.disk=nvme \
            --conf spark.kubernetes.driver.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict=false \
            --conf spark.kubernetes.driver.podTemplateFile='/opt/spark/conf/driver_pod_template.yml' \
            --conf spark.kubernetes.executor.podTemplateFile='/opt/spark/conf/executor_pod_template.yml' \
            --conf spark.kubernetes.driver.volumes.hostPath.spark-local-dir-1.mount.path='/pv/tmp' \
            --conf spark.kubernetes.driver.volumes.hostPath.spark-local-dir-1.options.path=/pv-disks/local \
            --conf spark.kubernetes.executor.volumes.hostPath.spark-local-dir-1.mount.path='/pv/tmp' \
            --conf spark.kubernetes.executor.volumes.hostPath.spark-local-dir-1.options.path=/pv-disks/local \
            --conf spark.local.dir='/pv/tmp' \
            --conf spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a=org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory \
            --conf spark.sql.sources.commitProtocolClass=org.apache.spark.internal.io.cloud.PathOutputCommitProtocol \
            --conf spark.sql.parquet.output.committer.class=org.apache.hadoop.mapreduce.lib.output.BindingPathOutputCommitter \
            --conf spark.hadoop.fs.s3a.committer.name=magic \
            --conf spark.hadoop.fs.s3a.committer.magic.enabled=true \
            --conf spark.hadoop.fs.s3a.fast.upload=true \
            local:///opt/spark/jars/eks-spark-benchmark-assembly-3.0.1.jar \
            \"s3a://gromav-test/tpcds-5tb\" \
            \"/opt/tpcds-kit/tools\" \
            \"parquet\" \
            \"5000\" \
            \"2000\" \
            \"true\" \
            \"true\" \
            \"true\""
          ]
          volumeMounts:
            - name: spark-pod-template
              mountPath: /opt/spark/conf/driver_pod_template.yml
              subPath: driver
            - name: spark-pod-template
              mountPath: /opt/spark/conf/executor_pod_template.yml
              subPath: executor
      serviceAccountName: spark
      restartPolicy: Never
      volumes:
        - name: spark-pod-template
          configMap:
            name: tpcds-data-generation-5tb-template
            defaultMode: 420
  backoffLimit: 4